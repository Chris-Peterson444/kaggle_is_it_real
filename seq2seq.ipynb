{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torchtext\n",
    "\n",
    "import spacy\n",
    "\n",
    "import sys, os, io, random, math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jupyterplot import ProgressPlot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Setup\n",
    "I am using spaCy to tokenize since it's a little more robust than the default pytorch tokenizer\n",
    "Seq2Seq reverses the input sentences, but since this techincally is more semantic analysis\n",
    "than sequence to sequence, we're going to leave it in the forward order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input_lang(text):\n",
    "    '''\n",
    "    Tokenize input from a string into a list of tokens and reverses it\n",
    "    '''\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def stoi(x):\n",
    "    return [int(x[0])]\n",
    "\n",
    "TEXT = Field(tokenize = tokenize_input_lang, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "LABEL = Field(dtype = torch.int, use_vocab=False, is_target=True, preprocessing=stoi)\n",
    "ID = Field(dtype = torch.int, use_vocab=False, preprocessing=stoi)\n",
    "# The original file contains: id    keyword    location    target\n",
    "# However I haven't decided how I want to include the keywords in this model yet (probably concatentaiton)\n",
    "# So for now it is being trained without\n",
    "#\n",
    "# To train with\n",
    "fields = [('id', ID), (None, None), (None,None), ('text', TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Loading Data To Use In PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "twitter_dataset = torchtext.data.TabularDataset('train.csv','csv',fields,skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.data.split() returns default a 70-30 split for training-testin\n",
    "# but since testing is provided by kaggle we will treat this as our \n",
    "# training-validation split\n",
    "train_data, valid_data = twitter_dataset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check we've loaded the right number and split correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total examples: 7613\n",
      "Number of training examples: 5329\n",
      "Number of validation examples: 2284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of total examples: {len(twitter_dataset.examples)}\")\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of one of the training data (Tokenized correctly and reversed (?) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [5647], 'text': ['@crabbycale', 'oh', 'my', 'god', 'the', 'memories', 'are', 'flooding', 'back'], 'label': [0]}\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[1]))\n",
    "print(type(vars(train_data.examples[1])['label'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slang Embeddings setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_embeddings(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "#     return data\n",
    "\n",
    "# slang_emb = load_embeddings('ud_embeddings/ud_basic.vec')\n",
    "slang_emb = torchtext.vocab.Vectors(name = '../chris_nlp_data/ud_embeddings/ud_basic.vec',\n",
    "                                   cache = '../chris_nlp_data/ud_embeddings',\n",
    "                                   unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helpful functions to verify things are working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next block from https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/B%20-%20A%20Closer%20Look%20at%20Word%20Embeddings.ipynb#scrollTo=DMkoy7iFMeN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(embeddings, word):\n",
    "    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n",
    "    return embeddings.vectors[embeddings.stoi[word]]\n",
    "\n",
    "def closest_words(embeddings, vector, n = 10):\n",
    "    \n",
    "    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n",
    "                 for word in embeddings.itos]\n",
    "    \n",
    "    return sorted(distances, key = lambda w: w[1])[:n]\n",
    "\n",
    "def print_tuples(tuples):\n",
    "    for w, d in tuples:\n",
    "        print(f'({d:02.04f}) {w}') \n",
    "        \n",
    "\n",
    "def analogy(embeddings, word1, word2, word3, n=5):\n",
    "    \n",
    "    #get vectors for each word\n",
    "    word1_vector = get_vector(embeddings, word1)\n",
    "    word2_vector = get_vector(embeddings, word2)\n",
    "    word3_vector = get_vector(embeddings, word3)\n",
    "    \n",
    "    #calculate analogy vector\n",
    "    analogy_vector = word2_vector - word1_vector + word3_vector\n",
    "    \n",
    "    #find closest words to analogy vector\n",
    "    candidate_words = closest_words(embeddings, analogy_vector, n+3)\n",
    "    \n",
    "    #filter out words already in analogy\n",
    "    candidate_words = [(word, dist) for (word, dist) in candidate_words \n",
    "                       if word not in [word1, word2, word3]][:n]\n",
    "    \n",
    "    print(f'{word1} is to {word2} as {word3} is to...')\n",
    "    \n",
    "    return candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_tuples(analogy(slang_emb, 'man', 'actor', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(twitter_dataset,\n",
    "                vectors = slang_emb)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in text vocabulary: 22637\n",
      "Unique tokens in label vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in text vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in label vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3042, 1: 2287})\n",
      "['<unk>', '<pad>', 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.freqs)\n",
    "print(LABEL.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "b = TEXT.vocab.vectors[TEXT.vocab.stoi['man']]\n",
    "a = get_vector(slang_emb,'man')\n",
    "print (a - b)\n",
    "# If loaded correctly all should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_iterator.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder from LSTM (Context vectors) + fully connected layer to predict sentiment\n",
    "\n",
    "The Encoder is based on Sew2Seq tutorial https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#scrollTo=Ao4yzOdnyv8s. \n",
    "\n",
    "Much of this math and logic will come from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessingWe're going to use a 2 layer LSTM for our encoder. If I have time I would like to do like Seq2Seq and do a 4 layer\n",
    "\n",
    "### Starting with simple RNN structure\n",
    "\n",
    "Consider the input sequence $X$ where $x_t \\in X$ is the input token to the first encoder layer at time t, and the hidden states $H = \\{h_1, h_2, ..., h_T\\}$ are the output a hidden layer. Also let $e(x)$ represent the embedding of the input token $x$. Then with superscripts representing the layer, we can consider our input functions as\n",
    "\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(e(x_t),h^1_{t-1})$$\n",
    "and\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h^1_t,h^2_{t-1})$$\n",
    "\n",
    "Let the initial hidden state as input for each layer be $h_0^l$ and the final context vector per layer be $z^l = h_T^l$\n",
    "\n",
    "### Transforming into LSTM\n",
    "\n",
    "We think about LSTM's being simple extensions from RNN's by adding an extra cell state an extra 'hidden state', although they have different functions. We denote this $c_t^l$ Our simple input function:\n",
    "\n",
    "$$h_t = \\text{RNN}(e(x_t), h_{t-1})$$\n",
    "\n",
    "can then be transformed into\n",
    "\n",
    "$$ (h_t, c_t) = \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})$$\n",
    "\n",
    "We'll also need an initial cell state $c_0^l$ and we will transform our context-vector/final-hidden-state to be the tupple $z^l = (h_T^l, c_T^l)$.\n",
    "\n",
    "By extension to two layers then we have \n",
    "\n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected\n",
    "\n",
    "Small fully connected layer to help with encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_dim, intermediate_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(input_dim, intermediate_dim)\n",
    "        self.activation = nn.functional.relu\n",
    "        self.fc_out = nn.Linear(intermediate_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        x = self.fc_in(input_x.squeeze(0))\n",
    "        x = self.activation(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, encoder, fc, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.fc = fc\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        fc_input = torch.cat((hidden[0],hidden[1],cell[0],cell[1]),1)\n",
    "        output = self.fc(fc_input)\n",
    "       \n",
    "        return output\n",
    "     \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "OUTPUT_DIM =  len(LABEL.vocab) - 2\n",
    "ENC_EMB_DIM = 300\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "\n",
    "FC_IN_DIM = HID_DIM * N_LAYERS * 2 # CELL and HIDDEN for each layer\n",
    "INTERMEDIATE_DIM = 128 # See what works best here\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "fc = FullyConnected(FC_IN_DIM,INTERMEDIATE_DIM,OUTPUT_DIM)\n",
    "\n",
    "embed_weights = TEXT.vocab.vectors\n",
    "enc.embedding.weight.data.copy_(embed_weights)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "enc.embedding.weight.data[UNK_IDX] = torch.zeros(ENC_EMB_DIM)\n",
    "enc.embedding.weight.data[PAD_IDX] = torch.zeros(ENC_EMB_DIM)\n",
    "\n",
    "#freeze embeddings\n",
    "enc.embedding.weight.requires_grad = False\n",
    "\n",
    "model = CustomModel(enc,fc, device ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights\n",
    "\n",
    "We're going to initialize all weights from a uniform distribution between -0.08 and +0.08 (like in the seq2seq paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info about the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,030,850 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function\n",
    "Could use squared loss but going to use cross-entropy loss for nowinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def loss_fn(self, label, predicted):\n",
    "#    return (label - predicted )^2\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.sin(i / 100)# Training loop\n",
    "At each iteration:\n",
    " - get tweet and label, $X$ and $Y$\n",
    " - zero gradients calculated from the last batch\n",
    " - feed the tweet and label into the model to get output $\\hat{y}$\n",
    " - flatten inputs with `.view` since loss function works with 2d inputsand 1d target\n",
    " - calculate the gradients with `loss.backward()`\n",
    " - clip the gradient to prevent explosion\n",
    " - update parameters with optimizer step\n",
    " - sum loss into running total\n",
    " \n",
    "Once done, return average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "#         print(batch)\n",
    "        src = batch.text\n",
    "#         print(\"src\",batch.text[:,0])\n",
    "        trg = batch.label\n",
    "#         print(\"trg\",batch.label)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src)\n",
    "#         print(trg[0,0])\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "#         print(output.shape)\n",
    "#         print(trg.shape)\n",
    "#         print(output)\n",
    "#         print(\"target\")\n",
    "#         print(trg)\n",
    "        output_dim = output.shape[-1]\n",
    "#         print(output_dim)\n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "#         print(len(trg))\n",
    "        trg = trg.long()\n",
    "        #trg = [(trg len ) * batch size]\n",
    "        #output = [(trg len ) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.text\n",
    "            trg = batch.label\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.view(-1, output_dim)\n",
    "            trg = trg.view(-1)\n",
    "            trg = trg.long()\n",
    "#             print(output[0].shape)\n",
    "#             print(trg[0])\n",
    "            #trg = [(trg len ) * batch size]\n",
    "            #output = [(trg len ) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "         \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how long an epoch takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "b_size = 52\n",
    "dim = 256\n",
    "file_name = str(N_EPOCHS)+\"-epochs-\"+str(b_size)+\"-batch-\"+str(dim)+\"-dim-full-embed\"\n",
    "training_loss_data = []\n",
    "validation_loss_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "CLIP = 1\n",
    "f = open(\"slang/log_data/\"+file_name+\".txt\",\"w\")\n",
    "best_valid_loss = float('inf')\n",
    "print(\"running\")\n",
    "pp = ProgressPlot(plot_names=['loss'],line_names=['training','validation'])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    validation_loss_data.append(valid_loss)\n",
    "    training_loss_data.append(train_loss)\n",
    "    pp.update([[train_loss,valid_loss]])\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"slang/models/\"+file_name+'.pt')\n",
    "    \n",
    "    f.write(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\\n')\n",
    "    f.write(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\\n')\n",
    "    f.write(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\\n')\n",
    "f.close()\n",
    "pp.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(training_loss_data)\n",
    "x_data = []\n",
    "for v in range(N_EPOCHS):\n",
    "    x_data.append(v+1)\n",
    "plt.plot(x_data,validation_loss_data)\n",
    "plt.plot(x_data,training_loss_data)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['valid','train'], loc=\"upper right\")\n",
    "plt.savefig(\"loss_plots/\"+file_name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data for putting in a table later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"loss_data/\"+file_name+\".txt\", \"w\")\n",
    "# write validation loss as an array\n",
    "for loss in validation_loss_data:\n",
    "    f.write(f'{loss:.3f}')\n",
    "    f.write(', ')\n",
    "f.write('\\n')\n",
    "# write train loss\n",
    "for loss in training_loss_data:\n",
    "    f.write(f'{loss:.3f}')\n",
    "    f.write(', ')\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"loss_data/\"+file_name+\".txt\", \"r\")\n",
    "s = f.readline()\n",
    "s = s.split(', ')\n",
    "s = s[:-1]\n",
    "test_loss = []\n",
    "for v in s:\n",
    "    test_loss.append(float(v))\n",
    "# print(validation_loss_data - test_loss)\n",
    "test_loss = np.array(test_loss)\n",
    "# print(test_loss)\n",
    "old_loss = np.array(validation_loss_data)\n",
    "print(old_loss - validation_loss_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = '30-epochs-10-batch-128-dim'\n",
    "model.load_state_dict(torch.load('new/slang/models/'+model_name+'.pt'))\n",
    "\n",
    "# test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ID = Field(dtype = torch.int, use_vocab=False, preprocessing=stoi)\n",
    "# The original file contains: id    keyword    location    target\n",
    "# However I haven't decided how I want to include the keywords in this model yet (probably concatentaiton)\n",
    "# So for now it is being trained without\n",
    "#\n",
    "# To train with\n",
    "fields = [('id', ID), (None, None), (None,None), ('text', TEXT), (None, None)]\n",
    "test_dataset = torchtext.data.TabularDataset('test.csv','csv',fields,skip_header=True)\n",
    "test_iterator = BucketIterator(\n",
    "    test_dataset, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_key = lambda x: len(x.id),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch.text\n",
    "            number = batch.id\n",
    "            \n",
    "            output = model(src)\n",
    "#             for word in src[:,0]:\n",
    "#                 print(TEXT.vocab.itos[word.item()])\n",
    "#             print(number[0][0].item())\n",
    "#             print(number[0].shape[0])\n",
    "            for j in range(number[0].shape[0]):\n",
    "#                 print(output[i,:].cpu().numpy())\n",
    "                results[number[0][j].item()] = output[j,:].cpu().numpy()\n",
    "                \n",
    "            \n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.view(-1, output_dim)\n",
    "            \n",
    "            \n",
    "\n",
    "            #trg = [(trg len ) * batch size]\n",
    "            #output = [(trg len ) * batch size, output dim]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "results = test_evaluate(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {}\n",
    "for key in results:\n",
    "    final_results[key] = np.argmax(results[key])\n",
    "# print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_results = sorted(final_results.items(), key = lambda item: item[0])\n",
    "# with open(\"slang_submission_2.csv\",\"w\") as f:\n",
    "#     f.write('id,target\\n')\n",
    "#     for n,t in sorted_results:\n",
    "#         f.write(str(n)+\",\"+str(t)+'\\n')\n",
    "\n",
    "def getPrecision(model, iterator):\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    precision = 0\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch.text\n",
    "            trg = batch.label\n",
    "            \n",
    "#             number = batch.id\n",
    "            \n",
    "            output = model(src)\n",
    "#             for word in src[:,0]:\n",
    "#                 print(TEXT.vocab.itos[word.item()])\n",
    "#             print(number[0][0].item())\n",
    "#             print(number[0].shape[0])\n",
    "#             print(output.shape)\n",
    "            for j in range(output.shape[0]):\n",
    "#                 print(output[i,:].cpu().numpy())\n",
    "#                 print(output.shape)\n",
    "                predicted = np.argmax(output[j,:].cpu().numpy())\n",
    "                expected = trg[0,j].item()\n",
    "                # If it was negative\n",
    "                if expected == 0:\n",
    "                    # if false positive\n",
    "                    if predicted == 1:\n",
    "                        false_pos = false_pos +1\n",
    "                \n",
    "                # if it was positive\n",
    "                elif expected == 1:\n",
    "                    # if true pos\n",
    "                    if predicted == 1:\n",
    "                        true_pos = true_pos + 1\n",
    "                \n",
    "                \n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            #trg = [(trg len ) * batch size]\n",
    "            #output = [(trg len ) * batch size, output dim]\n",
    "    return precision\n",
    "\n",
    "def getRecall(model, iterator):\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    recall = 0\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch.text\n",
    "            trg = batch.label\n",
    "            \n",
    "#             number = batch.id\n",
    "            \n",
    "            output = model(src)\n",
    "#             for word in src[:,0]:\n",
    "#                 print(TEXT.vocab.itos[word.item()])\n",
    "#             print(number[0][0].item())\n",
    "#             print(number[0].shape[0])\n",
    "#             print(output.shape)\n",
    "            for j in range(output.shape[0]):\n",
    "#                 print(output[i,:].cpu().numpy())\n",
    "#                 print(output.shape)\n",
    "                predicted = np.argmax(output[j,:].cpu().numpy())\n",
    "                expected = trg[0,j].item()\n",
    "                \n",
    "                # if it was positive\n",
    "                if expected == 1:\n",
    "                    # if true pos\n",
    "                    if predicted == 1:\n",
    "                        true_pos = true_pos + 1\n",
    "                    # if false neg\n",
    "                    elif predicted == 0:\n",
    "                        false_neg = false_neg + 1\n",
    "                \n",
    "                \n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "                \n",
    "                \n",
    "    return recall\n",
    "\n",
    "def getF1(precision, recall):\n",
    "    return 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janelle/software/miniconda2/envs/chris/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8637911464245176\n",
      "Recall: 0.7718052738336714\n",
      "F1 score: 0.8152115693626139\n"
     ]
    }
   ],
   "source": [
    "precision =  getPrecision(model,valid_iterator)\n",
    "recall = getRecall(model, valid_iterator)\n",
    "f1_score = getF1(precision, recall)\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 score: \"+str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
